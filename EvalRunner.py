import os
import time
import numpy as np
import hydra
import torch
import subprocess
import logging
import pandas as pd
import shutil
from datetime import datetime
from biotite.sequence.io import fasta
import GPUtil
from typing import Optional

from analysis import utils as au
from analysis import metrics
from data import utils as du
from omegaconf import DictConfig, OmegaConf
from openfold.data import data_transforms
import esm


class EvalRunner:

    def __init__(
        self,
        conf: DictConfig,
    ):

        self._log = logging.getLogger(__name__)
        self._conf = conf
        self._infer_conf = conf.inference
        # self._diff_conf = self._infer_conf.diffusion
        self._sample_conf = self._infer_conf.samples

        # Set random seed
        self._rng = np.random.default_rng(self._infer_conf.seed)

        # Set model hub directory for ESMFold.
        torch.hub.set_dir(self._infer_conf.pt_hub_dir)

        # Set-up accelerator
        if torch.cuda.is_available():
            available_gpus = "".join(
                [str(x) for x in GPUtil.getAvailable(order="memory", limit=8)]
            )
            self.device = f"cuda:{available_gpus[0]}"
        else:
            self.device = "cpu"
        self._log.info(f"Using device: {self.device}")

        self._pmpnn_dir = self._infer_conf.pmpnn_dir

        # Load ESMFold model
        self._folding_model = esm.pretrained.esmfold_v1().eval()
        self._folding_model = self._folding_model.to(self.device)

    def pdbTM(
        self,
        input: Union[str, Path],
        foldseek_database_path: Union[str, Path],
        process_id: int,
        save_tmp: bool = False,
        foldseek_path: Optional[Union[Path, str]] = None,
    ) -> Union[float, dict]:
        """
        Calculate pdbTM values with a customized set of parameters by Foldseek.

        Args:
        `input`: Input PDB file or csv file containing PDB paths.
        `process_id`: Used for saving temporary files generated by Foldseek.
        `save_tmp`: If True, save tmp files generated by Foldseek, otherwise deleted after calculation.
        `foldseek_path`: Path of Foldseek binary file for executing the calculations.
                        If you've already installed Foldseek through conda, just use "foldseek"
                        instead of this path.

        CMD args:
        `pdb100`: Path of PDB database created compatible with Foldseek format.
        `output_file`: .m8 file containing Foldseek search results. Deleted if `save_tmp` = False.
        `tmp`: Temporary path when running Foldseek.
        For other CMD parameters and usage, we suggest users go to Foldseek official website
        (https://github.com/steineggerlab/foldseek) or type `foldseek easy-search -h` for detailed information.

        Returns:
        `top_pdbTM`: The highest pdbTM value among the top three targets hit by Foldseek.
        """
        # Handling multiprocessing
        base_tmp_path = "../tmp/"
        tmp_path = os.path.join(base_tmp_path, f"process_{process_id}")
        os.makedirs(tmp_path, exist_ok=True)

        # pdb100 = "~/zzq/foldseek/database/pdb100/pdb"
        # Check whether input is a directory or a single file
        if ".pdb" in input:
            output_file = f"./{os.path.basename(input)}.m8"

            cmd = f"foldseek easy-search \
                    {input} \
                    {foldseek_database_path} \
                    {output_file} \
                    {tmp_path} \
                    --format-mode 4 \
                    --format-output query,target,evalue,alntmscore,rmsd,prob \
                    --alignment-type 1 \
                    --num-iterations 2 \
                    -e inf \
                    -v 0"

            if foldseek_path is not None:
                cmd.replace("foldseek", {foldseek_path})

            _ = subprocess.run(
                cmd,
                shell=True,
                check=True,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
            )

            result = pd.read_csv(output_file, sep="\t")
            top_pdbTM = round(result["alntmscore"].head(1).max(), 3)

            if save_tmp == False:
                os.remove(output_file)

        else:
            return None

        return top_pdbTM

    def calculate_novelty(
        self,
        input_csv: Union[str, Path, pd.DataFrame],
        foldseek_database_path: Union[str, Path],
        max_workers: int,
        cpu_threshold: float,
    ) -> pd.DataFrame:
        """
        Novelty Calculation.

        We provide two options for calculating the pdbTM value:
        1. Indepentdent Mode: Calculate the pdbTM value of a single input PDB file.
            Example usage: python pdbTM.py -c {example}.pdb
        2. Batch Mode: Calculate pdbTM of a number of PDBs once in a time.
            Take a csv file with 'backbone_path' as the path of PDB files as input.
            A csv file will be returned with 'pdbTM' column filled with corresponding values.

        Args:

        Independent Mode:
        [Required]
        '-c', '--calculate': Path of PDB file you want to calculate pdbTM value of.

        Batch Mode:
        [Required]
        '-i', '--input': Path of input csv file you want to calculate with.
        [Optional]
        '-o', '--output': Path of output csv file with calculated pdbTM values.
                            Default = "novelty_results.csv"
        """

        df = (
            pd.read_csv(input_csv).copy()
            if isinstance(input_csv, str) or isinstance(input_csv, Path)
            else input_csv.copy()
        )
        if "pdbTM" not in df.columns:
            df.loc[:, "pdbTM"] = None

        futures = {}
        if max_workers > 0:
            with ProcessPoolExecutor(max_workers=max_workers) as executor:
                process_id = 0
                for backbone_path in df["backbone_path"].unique():
                    if pd.isna(
                        df[df["backbone_path"] == backbone_path]["pdbTM"].iloc[0]
                    ):
                        while psutil.cpu_percent(interval=1) > cpu_threshold:
                            time.sleep(0.5)
                        future = executor.submit(
                            pdbTM, backbone_path, foldseek_database_path, process_id
                        )
                        futures[future] = backbone_path
                        process_id += 1

                for future in as_completed(futures):
                    pdbTM_value = future.result()
                    backbone_path = futures[future]
                    df.loc[df["backbone_path"] == backbone_path, "pdbTM"] = pdbTM_value

                # df['pdbTM'] = df['backbone_path'].apply(lambda x: pdbTM_values[x])
        else:
            for process_id_placeholder, backbone_path in enumerate(
                df["backbone_path"].unique()
            ):
                pdbTM_value = pdbTM(
                    backbone_path, foldseek_database_path, process_id_placeholder
                )
                df.loc[df["backbone_path"] == backbone_path, "pdbTM"] = pdbTM_value

        return df

    def calc_mdtraj_metrics(self, pdb_path: str):
        try:
            traj = md.load(pdb_path)
            pdb_ss = md.compute_dssp(traj, simplified=True)
            pdb_coil_percent = np.mean(pdb_ss == "C")
            pdb_helix_percent = np.mean(pdb_ss == "H")
            pdb_strand_percent = np.mean(pdb_ss == "E")
            pdb_ss_percent = pdb_helix_percent + pdb_strand_percent
            pdb_rg = md.compute_rg(traj)[0]
        except IndexError as e:
            print("Error in calc_mdtraj_metrics: {}".format(e))
            pdb_ss_percent = 0.0
            pdb_coil_percent = 0.0
            pdb_helix_percent = 0.0
            pdb_strand_percent = 0.0
            pdb_rg = 0.0
        return {
            "non_coil_percent": pdb_ss_percent,
            "coil_percent": pdb_coil_percent,  # coil
            "helix_percent": pdb_helix_percent,  # alpha helix
            "strand_percent": pdb_strand_percent,  # beta sheet
            "radius_of_gyration": pdb_rg,
        }

    def run_max_cluster(self, designable_file_path, designable_dir):
        pmpnn_args = [
            "./maxcluster64bit",
            "-l",
            designable_file_path,
            os.path.join(designable_dir, "all_by_all_lite"),
            "-C",
            "2",
            "-in",
            "-Rl",
            os.path.join(designable_dir, "tm_results.txt"),
            "-Tm",
            "0.5",
        ]
        process = subprocess.Popen(
            pmpnn_args, stdout=subprocess.PIPE, stderr=subprocess.PIPE
        )
        stdout, _ = process.communicate()

        # Extract number of clusters
        match = re.search(
            r"INFO\s*:\s*(\d+)\s+Clusters\s+@\s+Threshold\s+(\d+\.\d+)\s+\(\d+\.\d+\)",
            stdout.decode("utf-8"),
        )
        clusters = int(match.group(1))
        cluster_results_path = os.path.join(designable_dir, "cluster_results.txt")
        with open(cluster_results_path, "w") as f:
            f.write(stdout.decode("utf-8"))

        # Extract cluster centroids
        cluster_lines = stdout.decode("utf-8").split("\n")
        centroid_section = False
        for line in cluster_lines:
            if "Centroids" in line:
                centroid_section = True
            if centroid_section:
                match = re.search(r"(?<=\s)(\/[^\s]+\.pdb)", line)
                if match is not None:
                    centroid_path = match.group(1)
                    copy_name = centroid_path.split("/")[-2] + ".pdb"
                    shutil.copy(centroid_path, os.path.join(designable_dir, copy_name))
        return clusters

    def extract_clusters_from_maxcluster_out(self, file_path):
        # Extracts cluster information from the stdout of a maxcluster run
        cluster_to_paths = {}
        paths_to_cluster = {}
        read_mode = False
        with open(file_path) as file:
            lines = file.readlines()
            for line in lines:
                if line == "INFO  : Item     Cluster\n":
                    read_mode = True
                    continue

                if line == "INFO  : ======================================\n":
                    read_mode = False

                if read_mode:
                    # Define a regex pattern to match the second number and the path
                    pattern = r"INFO\s+:\s+\d+\s:\s+(\d+)\s+(\S+)"

                    # Use re.search to find the first match in the string
                    match = re.search(pattern, line)

                    # Check if a match is found
                    if match:
                        # Extract the second number and the path
                        cluster_id = match.group(1)
                        path = match.group(2)
                        if cluster_id not in cluster_to_paths:
                            cluster_to_paths[cluster_id] = [path]
                        else:
                            cluster_to_paths[cluster_id].append(path)
                        paths_to_cluster[path] = cluster_id

                    else:
                        raise ValueError(f"Could not parse line: {line}")

        return cluster_to_paths, paths_to_cluster

    def get_reward(
        self,
        decoy_pdb_dir: str,
        reference_pdb_path: str,
    ):
        """Run self-consistency on design proteins against reference protein.

        Args:
            decoy_pdb_dir: directory where designed protein files are stored.
            reference_pdb_path: path to reference protein file

        Returns:
            Writes ProteinMPNN outputs to decoy_pdb_dir/seqs
            Writes ESMFold outputs to decoy_pdb_dir/esmf
            Writes results in decoy_pdb_dir/sc_results.csv
        """

        # Run PorteinMPNN
        output_path = os.path.join(decoy_pdb_dir, "parsed_pdbs.jsonl")
        process = subprocess.Popen(
            [
                "python",
                f"{self._pmpnn_dir}/helper_scripts/parse_multiple_chains.py",
                f"--input_path={reference_pdb_path}",
                f"--output_path={output_path}",
            ]
        )
        _ = process.wait()
        num_tries = 0
        ret = -1
        pmpnn_args = [
            "python",
            f"{self._pmpnn_dir}/protein_mpnn_run.py",
            "--out_folder",
            decoy_pdb_dir,
            "--jsonl_path",
            output_path,
            "--num_seq_per_target",
            "3",
            "--sampling_temp",
            "0.1",
            "--seed",
            "38",
            "--batch_size",
            "1",
        ]
        # if self._infer_conf.gpu_id is not None:
        #     pmpnn_args.append('--device')
        #     pmpnn_args.append(str(self._infer_conf.gpu_id))
        while ret < 0:
            try:
                process = subprocess.Popen(
                    pmpnn_args, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT
                )
                ret = process.wait()
            except Exception as e:
                num_tries += 1
                self._log.info(f"Failed ProteinMPNN. Attempt {num_tries}/5")
                torch.cuda.empty_cache()
                if num_tries > 4:
                    raise e
        mpnn_fasta_path = os.path.join(decoy_pdb_dir, "seqs", "sample.fa")

        # Run ESMFold on each ProteinMPNN sequence and calculate metrics.
        mpnn_results = {
            "tm_score": [],
            "sample_path": [],
            "header": [],
            "sequence": [],
        }

        esmf_dir = os.path.join(decoy_pdb_dir, "esmf")
        os.makedirs(esmf_dir, exist_ok=True)
        fasta_seqs = fasta.FastaFile.read(mpnn_fasta_path)
        sample_feats = du.parse_pdb_feats(
            "sample.pdb", os.path.join(reference_pdb_path, "sample.pdb")
        )
        for i, (header, string) in enumerate(fasta_seqs.items()):

            # Run ESMFold
            esmf_sample_path = os.path.join(esmf_dir, f"sample_{i}.pdb")
            _ = self.run_folding(string, esmf_sample_path)
            esmf_feats = du.parse_pdb_feats("folded_sample", esmf_sample_path)
            sample_seq = du.aatype_to_seq(sample_feats["aatype"])

            # Calculate scTM of ESMFold outputs with reference protein
            _, tm_score = metrics.calc_tm_score(
                sample_feats["bb_positions"],
                esmf_feats["bb_positions"],
                sample_seq,
                sample_seq,
            )

            mpnn_results["tm_score"].append(tm_score)
            mpnn_results["sample_path"].append(esmf_sample_path)
            mpnn_results["header"].append(header)
            mpnn_results["sequence"].append(string)

        # Save results to CSV
        csv_path = os.path.join(decoy_pdb_dir, "sc_results.csv")
        print(mpnn_results)
        mpnn_results = pd.DataFrame(mpnn_results)
        mpnn_results.to_csv(csv_path)

        # calc substructure ratio
        pdb_path = os.path.join(decoy_pdb_dir, "sample.pdb")
        calc_ratio = self.calc_mdtraj_metrics(pdb_path)
        # save to csv, for debugging
        df = pd.DataFrame(calc_ratio, index=[0])

        designability_reward = mpnn_results["tm_score"].mean()
        return designability_reward

    def run_folding(self, sequence, save_path):
        """Run ESMFold on sequence."""
        with torch.no_grad():
            output = self._folding_model.infer_pdb(sequence)

        with open(save_path, "w") as f:
            f.write(output)
        return output

    def get_designability_rewards(self, pdb_csv_path):
        """Get designability reward from csv file.

        pdb_csv_path : str
            Path to the csv file containing the pdb paths
        """

        pdb_path_list = pd.read_csv(pdb_csv_path)
        rewards = []
        for pdb_path in pdb_path_list:
            sc_output_dir = os.path.join(pdb_path, "self_consistency")
            os.makedirs(sc_output_dir, exist_ok=True)
            reward = self.get_reward(sc_output_dir, pdb_path)
            rewards.append(reward)

        return rewards


@hydra.main(version_base=None, config_path="configs", config_name="_inference")
def run(conf: DictConfig) -> None:

    print("Test designability")
    start_time = time.time()

    # Example pdb path
    pdb_path = "/home/shuaikes/Project/protein-evaluation-notebook/example_data/length_70/sample_0/"
    sc_output_dir = os.path.join(pdb_path, "self_consistency")
    os.makedirs(sc_output_dir, exist_ok=True)

    # run reward model
    Reward_model = EvalRunner(conf)
    reward = Reward_model.get_reward(sc_output_dir, pdb_path)

    # run reward model based on csv file
    # pdb_csv_path = "/home/shuaikes/server2/shuaikes/projects/frameflow_rl/inference_outputs/weights/pdb/published/unconditional/run_2024-10-21_01-26-05/length_70/sample_1/pdb_paths.csv"
    # reward = Reward_model.get_designability_rewards(pdb_csv_path)

    elapsed_time = time.time() - start_time
    print(f"Finished in {elapsed_time:.2f}s")
    print(reward)


if __name__ == "__main__":
    run()
